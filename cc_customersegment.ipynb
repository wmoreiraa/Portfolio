{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83901583-2695-435d-9349-4da63b5f43aa",
   "metadata": {},
   "source": [
    "# Credit Card Customer Segmentation\n",
    "\n",
    "## The problem\n",
    "\n",
    "A marketing strategy team is trying to buid new [personas](https://www.interaction-design.org/literature/article/personas-why-and-how-you-should-use-them) so the marketing campaing is more accurate and achieve a higher return on investment (ROI).\n",
    "\n",
    "Saying this, the problem can be described as a customer segmentation task. \n",
    "Hence, to tackle the problem we will try to follow the suitable steps of a well known data science methodology - CRISP.\n",
    "\n",
    "1. **Understand the  business problem**; \n",
    "2. **Understand the data**;\n",
    "3. **Prepare the data**;\n",
    "4. **Modeling**;\n",
    "5. **Evaluation**;\n",
    "6. **Deploy** (Of course, for this problem, we will not deploy any model. Altough, this project may evolve in the future)\n",
    "\n",
    "\n",
    "## The Data\n",
    "The data summarizes the usage of 9000 credit card holders during 6 months, so the variable intend to capture the behavioural of those customers.\n",
    "\n",
    "- **CUSTID** : Identification of Credit Card holder. (ID)\n",
    "- **BALANCE** : Balance amount left in their account to make purchases \n",
    "- **BALANCEFREQUENCY** : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n",
    "- **PURCHASES** : Amount of purchases made from account\n",
    "- **ONEOFFPURCHASES** : Maximum purchase amount done in one-go\n",
    "- **INSTALLMENTSPURCHASES** : Amount of purchase done in installment\n",
    "- **CASHADVANCE** : Cash in advance given by the user\n",
    "- **PURCHASESFREQUENCY** : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n",
    "- **ONEOFFPURCHASESFREQUENCY** : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n",
    "- **PURCHASESINSTALLMENTSFREQUENCY** : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n",
    "- **CASHADVANCEFREQUENCY** : How frequently the cash in advance being paid\n",
    "- **CASHADVANCETRX** : Number of Transactions made with \"Cash in Advanced\"\n",
    "- **PURCHASESTRX** : Number of purchase transactions made\n",
    "- **CREDITLIMIT** : Limit of Credit Card for user\n",
    "- **PAYMENTS** : Amount of Payment done by user\n",
    "- **MINIMUM_PAYMENTS** : Minimum amount of payments made by user\n",
    "- **PRCFULLPAYMENT** : Percent of full payment paid by user\n",
    "- **TENURE** : Tenure of credit card service for user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612e3ca-1c65-41b3-9fc7-91c52d6f0a22",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd97e9-7179-4893-bc5d-497e7ad6355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os / sys \n",
    "import os \n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "\n",
    "# standard\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "# stats \n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# machine learning\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e717a-44fd-468a-9105-f6ebbf669ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global configs\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme()\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d53ca3-6cf6-4772-bc7c-2382b3a3d705",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979a6dc-b935-46ff-adae-8ca6bc00768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob('raw/*.csv')\n",
    "\n",
    "raw_df = pd.read_csv(file[0], index_col = 'CUST_ID')\n",
    "\n",
    "#drop duplicates\n",
    "raw_df.drop_duplicates(inplace = True, ignore_index = True)\n",
    "\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b8bc7-79e3-42a7-991f-0d7c4868cc60",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "After this section of the project, those are the criteria that needs to be met:\n",
    "- Missing Values\n",
    "- Columns Type\n",
    "- Distributions\n",
    "- Relationship between columns ( correlation, interaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb308299-1a01-46be-9335-a708b71e5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "def check_missing(data):\n",
    "    \n",
    "    # columns that have NA\n",
    "    mask_columns_na = raw_df.columns[raw_df.isna().any()]\n",
    "    \n",
    "    # Value counts of NA\n",
    "    na_count = raw_df[mask_columns_na].isna().sum()\n",
    "    \n",
    "    percentage_na = (raw_df[mask_columns_na].isna().sum()/len(raw_df)) * 100\n",
    "    df_na = pd.DataFrame({'NA Values': na_count, 'Percentage NA': percentage_na})\n",
    "    df_na = df_na.sort_values(by = 'Percentage NA', ascending = False)\n",
    "    \n",
    "    return print(df_na)\n",
    "\n",
    "check_missing(raw_df)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e94eb-b0a6-4c5c-9a3f-d8ef2edcffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payments\n",
    "plt.figure(figsize= (16,6))\n",
    "sns.histplot(raw_df, x = 'MINIMUM_PAYMENTS', log_scale = True)\n",
    "plt.show()\n",
    "\n",
    "print('-'*50)\n",
    "print( 'The number of CC holders that havent paid anything is: {}'.format(sum(raw_df['MINIMUM_PAYMENTS'] == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2daa5a2-6973-4e9f-a438-0aa3474ec6fa",
   "metadata": {},
   "source": [
    "Well, the hypothesis of having zero cc holders that havent paid anything doensn't seems plausible to me, so lets give it a fast look of some of those NA occurrences.\n",
    "To be more precise, we're looking for behaviour consistent with not paying anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021add6-568b-4245-a45c-4ed769ccf82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[raw_df['MINIMUM_PAYMENTS'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c766d-254b-4394-87cf-6eaf17d6c83f",
   "metadata": {},
   "source": [
    "Okey, so it seem's my hypothesis is 80% correct. As we can see, a lot of rows that have NA's the paymements columns is equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f31f8e-f539-4671-868e-528b4f63ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_zero_pay = raw_df['PAYMENTS'] == 0.0 \n",
    "raw_df.loc[mask_zero_pay, 'MINIMUM_PAYMENTS'] = raw_df.loc[mask_zero_pay, 'MINIMUM_PAYMENTS'].fillna(0)\n",
    "    \n",
    "check_missing(raw_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ac344-f96e-480a-a9fd-09008ac50b64",
   "metadata": {},
   "source": [
    "Well, now we have less than 1% of the data missing, so we have two options:\n",
    "\n",
    "- Undertand why and imput with some descriptive stats (mean, median)\n",
    "- drop \n",
    "\n",
    "In practice, this a tradeoff between time (money) x return. I don't think that of less than 1% of the data would give me a better return on time. \n",
    "I will drop those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31e0aa-3e7f-4692-989a-fb168ee31ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.dropna(how = 'any', axis = 0, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8b777-f73f-45ab-806a-4433f353dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['TENURE'] = raw_df['TENURE'].astype('category')\n",
    "print(raw_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6e96e-5431-47af-a3b6-ee731a8522d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fdd89-9130-4441-af33-b78c315494b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde(n, df):\n",
    "    \"\"\" \n",
    "    Plot kde for n columns \n",
    "\n",
    "    Input\n",
    "    n =  number of columns\n",
    "    df = data frame\n",
    "    Output\n",
    "    Return - None\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,18))\n",
    "    for i in range(0,n):\n",
    "        plt.subplot(6,3,i+1)\n",
    "        sns.kdeplot(df[df.columns[i]])\n",
    "        plt.title(df.columns[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde(n=16, df = raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0023bd2-00c4-4eaa-b696-152d817d6396",
   "metadata": {},
   "source": [
    "Not surprisingly the data doenst follow a normal distribution and it follow the Power-Law (80-20) Rule. \n",
    "Hence, we will need some transformation to reduce the skewness.\n",
    "- Categorical: Tenure \n",
    "- Left: Balance frequency \n",
    "- Right: All except above.\n",
    "\n",
    "_Note : All the data inputed is positive, but there are zero-values, so we will need to use log(x+1)_\n"
   ]
  },
  {
   "source": [
    "# Reducing the skewness through log transformation\n",
    "\n",
    "cols = raw_df.columns.difference(['TENURE', 'BALANCE_FREQUENCY'])\n",
    "\n",
    "for col in cols:\n",
    "    raw_df[col] = np.log(1 + raw_df[col])\n",
    "\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde(n=16, df= raw_df)"
   ]
  },
  {
   "source": [
    "Well, it's still not normal distributed but the data is way better than the previously situation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d30188-8a6b-4240-88f8-b8dd343a1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,12))\n",
    "sns.heatmap(raw_df.corr(), cmap = 'coolwarm', linewidths = .5, annot = True, vmin = -1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdeeb87-f259-47ff-8b49-5135ab598206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.get_dummies(raw_df, columns = ['TENURE'], drop_first= True)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53d66b-e4c4-41f8-bdae-1bd41f6ca1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df = raw_df.copy()\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd931c6e-ee61-4c0f-8f10-1d5e3a2e4105",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093d2d8-0707-44cf-a600-54d0c3c4ceb6",
   "metadata": {},
   "source": [
    "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. I.e, less variables without losing too much information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec101f-0ddd-4e0e-af6c-4cd3a11720f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the ideal number of components by the variance explained\n",
    "pca = PCA().fit(df)\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker = 'o', linestyle = '--')\n",
    "plt.xlabel('Number of components')\n",
    "plt.xticks([i for i in range(23)])\n",
    "plt.ylabel('Cumulative variance explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e497d78-0ae5-41e7-90a5-22ae688050d2",
   "metadata": {},
   "source": [
    "Hence, I will select the 90% mark, which is 6 components.\n",
    "Below we will just give it a look how the importance of the features are distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1cc450-8be8-4bb8-b5cb-4547016805cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, value in enumerate(list(pca.explained_variance_)):\n",
    "    print('Explained variance - PCA {comp}: {value}'.format(comp = i+1, value = value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df713d-4524-4e58-bba7-51a7b2b49829",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_red = PCA(0.9).fit_transform(df)\n",
    "X_red\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f7e7c-d91b-403f-94a8-4d555bd038de",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801dd7a-6771-4306-93b8-7f8cb8d901dd",
   "metadata": {},
   "source": [
    "We will use the silhouette plot and score to choose the otimal number of clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e2a75-4407-4980-8c26-15fb79c1ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_ploter(array, upper_range):\n",
    "    ''' Input array and upper limit of cluster to iterate over '''\n",
    "    \n",
    "    range_n_clusters = range(2,upper_range)\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(16, 6)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1\n",
    "        ax1.set_xlim([-1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(array) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=23, n_jobs = 4)\n",
    "        cluster_labels = clusterer.fit_predict(array)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(array, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(array, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-1,-0.8, -0.6, -0.4,-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(array[:, 0], array[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors, edgecolor='k')\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd30ed5-bab8-4f50-be4d-a1a2111686a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_ploter(X_red, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6016300b-65c6-4d67-b82b-73431f85112c",
   "metadata": {},
   "source": [
    "From the silhouette plots and score we can see that the appropriated cluster number is equal to 3, since the avg score is higher and there are not negative coefficients.\n",
    "Let's use the elbow method as well and see if we get any surprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c645b53-52e0-41f9-a29c-1adb40db3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_plotter(array, upper_limit):\n",
    "    distortions = []\n",
    "    k = range(2,upper_limit)\n",
    "    for n_clusters in k:\n",
    "        kmeanModel = KMeans(n_clusters=n_clusters, n_jobs = 4, random_state = 23 )\n",
    "        kmeanModel.fit(array)\n",
    "        distortions.append(kmeanModel.inertia_)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(k, distortions, marker = 'o', linestyle = '--')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8df826-f4d7-4032-8407-4db4614ec18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_plotter(X_red, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc33748-9ae9-430b-bfad-93f9fb9824e6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0eab7-b9af-4177-9512-b6a19bcac5d8",
   "metadata": {},
   "source": [
    "As for the first iteration of the model, we've seen that the even though we're using the 0,9 threshold for cumulative variance explained.\n",
    "\n",
    "So, looking at the silhouette plot and the scatter plot in 2 dimensions, the most promising number of cluster it's 3 or 2. Honestly, I don't think only 2 cluster will help to solve the business problem, so lets check both on the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180ce6a-d40c-4f01-ab87-39cb956e905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=2 , random_state=23, n_jobs =4)\n",
    "kmeans_3 = KMeans(n_clusters=3, n_jobs=4, random_state = 23)\n",
    "\n",
    "kmeans_2.fit(X_red)\n",
    "kmeans_3.fit(X_red)\n",
    "\n",
    "print('Silhoutte score of our model  with 2 cluster is ' + str(silhouette_score(X_red, kmeans_2.labels_)))\n",
    "print('Silhoutte score of our model  with 3 cluster is ' + str(silhouette_score(X_red, kmeans_3.labels_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster_2'] = kmeans_2.labels_\n",
    "df['cluster_3'] = kmeans_3.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    df[col] = np.exp(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cols_2 = [\"BALANCE\", \"PURCHASES\",\"PURCHASES_FREQUENCY\", \"CASH_ADVANCE\",\"CREDIT_LIMIT\", \"PAYMENTS\", \"MINIMUM_PAYMENTS\", 'cluster_2']\n",
    "best_cols_3 = [\"BALANCE\", \"PURCHASES\", \"PURCHASES_FREQUENCY\", \"CASH_ADVANCE\",\"CREDIT_LIMIT\", \"PAYMENTS\", \"MINIMUM_PAYMENTS\", 'cluster_3']\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1cbfd-a599-4ff6-a833-57ba7f3407f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20)) \n",
    "sns.pairplot(data = df[best_cols_2], hue = 'cluster_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20)) \n",
    "sns.pairplot(data = df[best_cols_3], hue = 'cluster_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58485b2-26e7-4539-9a11-5d993934a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(data=df, x='CREDIT_LIMIT', y='PURCHASES', hue='cluster_3')\n",
    "plt.title('Distribution of clusters based on Credit limit and total purchases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0452f-e712-4375-8f77-16b9c1fd5cae",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13663ef-c8a5-49b1-86f8-91b42cd60f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd067e582461fcab25656fdfe2355fb136d36de527ac19eb4a1481ac02dc70ccd47",
   "display_name": "Python 3.9.2 64-bit ('workbench': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}